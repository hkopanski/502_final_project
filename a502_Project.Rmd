---
title: "ST502 Project"
author: "Halid Kopanski and Justin Feathers"
geometry: "left = 1.75 cm, right = 1.75 cm, top = 1.25 cm, bottom = 2 cm"
#geometry: "top = 1.25 cm, bottom = 2 cm"
output:
    pdf_document: default
    word_document: default
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Use required packages
library(tidyverse) #for plots and data manipulation
library(cowplot) #aligning plots
library(gridExtra)
library(scales)

df_data <- read_csv("framingham_data.csv") # Read in data
```

## Part I

### Framingham Heart Study

The Framingham data set contains the systolic blood pressure of `r nrow(df_data)` smokers and nonsmokers. For this study we will assume the data are random samples from normal distributions (see Appendix I for more details) where $\mu_{1}$ and $\sigma^{2}_{1}$ are the mean and variance for nonsmokers and  $\mu_{2}$ and $\sigma^{2}_{2}$ are the mean and variance for smokers. The null hypothesis, H~0~: $\mu$~1~ - $\mu$~2~ = 0, states there is no significant difference in blood pressure between smokers and nonsmokers. We wish to prove the alternative hypothesis, H~A~: $\mu$~1~ - $\mu$~2~ $\ne$ 0, which states there is a significant difference in the means. We will accomplish this through hypothesis testing to determine if enough evidence does, indeed, exist to reject the null hypothesis.

### Methods

We conducted the analysis under two conditions: the first assumed equal population variance (using pooled sample variance, $S^{2}_{p}$) and the second assumed unequal population variance. The results will be presented and any discrepancies between the two outcomes will be discussed.

```{r, echo = FALSE, fig.dim = c(7, 2.5), fig.align = 'center'}
df_data$index <- seq(nrow(df_data)) # Add an index column

#df_data %>% summary # Summarize Data

# Split data into smoker and nonsmoker
df_smoker <- df_data %>% filter(currentSmoker == 1)
df_nonsmoker <- df_data %>% filter(currentSmoker == 0)

#Create a sample variance function to ensure proper calculation
sample_variance <- function(x, sampling = TRUE){
    if (sampling == TRUE){
        sum((x - mean(x))^2) / (length(x) - 1)
    } else if(sampling == FALSE) {
        sum((x - mean(x))^2) / (length(x))
    }
}
#Create pooled sample variance function 
f_pooled_variance <- function(x, y){
    ((length(x) - 1) * sample_variance(x) + 
     (length(y) - 1) * sample_variance(y)) / 
    (length(x) + length(y) - 2)
}

# Skewness function 
skew_function <- function(x){
    mean((x - mean(x))^3) / sqrt(sample_variance(x))^3
}

# kurtosis function
kurt_function <- function(x){
    mean((x - mean(x))^4) / sqrt(sample_variance(x))^4
}

# Create a Satterthawaite Approximation Function

satterth <- function(s1, s2, n1, n2){
    term1 <- s1/n1
    term2 <- s2/n2
    nu <- (term1 + term2)^2 / ((term1^2/(n1 - 1)) + (term2^2/(n2 - 1)))
    return(floor(nu))
}


#Plot and compare split data

#options(repr.plot.width = 6, repr.plot.height = 4, repr.plot.res = 150)

plot_colors <- c("#001427","#708d81","#f4d58d","#bf0603","#8d0801")
y_limits <- c(0, 0.0225)

total_data <- ggplot(df_data) + geom_density(aes(sysBP), 
                                             fill = plot_colors[1],
                                             alpha = 0.6) +
                       ylim(y_limits) + ylab("Density") + xlab("")

sep_data <- ggplot() + geom_density(data = df_smoker, aes(sysBP), 
                                    fill = plot_colors[3], alpha = 0.6) + 
                       geom_density(data = df_nonsmoker, aes(sysBP), 
                                    fill = plot_colors[5], alpha = 0.6) +
                       ylim(y_limits) + ylab("") + xlab("Systolic Blood Pressure")

plot_3 <- ggplot() + geom_density(data = df_smoker, aes(sysBP, 
                                    fill = plot_colors[3]), alpha = 0.5) + 
                       geom_density(data = df_data, aes(sysBP, 
                                    fill = plot_colors[1]), alpha = 0.5) +
                       geom_density(data = df_nonsmoker, aes(sysBP, 
                                    fill = plot_colors[5]), alpha = 0.5) +
                       ylim(y_limits) + ylab("") + xlab("") + 
                       scale_fill_manual("",
                                         values = plot_colors[c(1, 5, 3)], 
                                         labels = c("Total", "Non Smoker", "Smoker")) +
                       theme(legend.position = c(0.8, 0.9),
                             legend.text = element_text(size = 6),
                             legend.key.height = unit(0.25, 'cm'),
                             legend.key.width = unit(0.25, 'cm'))

#plot_grid(total_data, sep_data, plot_3, align = 'vh', 
          #hjust = -1,nrow = 2, ncol = 2)

data_kurtosis <- kurt_function(df_data$sysBP)
data_skew <- skew_function(df_data$sysBP)
data_IQR <- as.numeric(quantile(df_data$sysBP, probs = 0.75)) - 
       as.numeric(quantile(df_data$sysBP, probs = 0.25))
data_MAD <- median(abs(df_data$sysBP - median(df_data$sysBP)))
data_samVar <- sample_variance(df_data$sysBP)

eIQR <- data_IQR / 1.35
eMAD <- data_MAD / 0.675

# Q-Q Plot

data_qqplot <- 
ggplot(df_data, aes(sample = sysBP)) + 
stat_qq(shape = 1) + stat_qq_line() + 
ggtitle("Normal Q-Q Plot for Blood Pressure Data") + 
xlab("Theoretical Quantiles") +
ylab("Sample Quantiles")
```

### Analysis

```{r, echo = FALSE}

# Common values for analysis

alpha <- 0.05

mu_smoker <- mean(df_smoker$sysBP)
var_smoker <- sample_variance(df_smoker$sysBP)
n_smoker <- length(df_smoker$sysBP)

mu_nonsmoker <- mean(df_nonsmoker$sysBP)
var_nonsmoker <- sample_variance(df_nonsmoker$sysBP)
n_nonsmoker <- length(df_nonsmoker$sysBP)

# Two Sample T-test - Pooled Sample Variance - P-value

dof_1 <- (n_smoker + n_nonsmoker - 2)

p_sample_var_1 <- f_pooled_variance(df_smoker$sysBP, 
                                    df_nonsmoker$sysBP)
p_sample_var_w <- sqrt(p_sample_var_1/n_smoker + p_sample_var_1/n_nonsmoker)

t_obs_1 <- (mu_smoker - mu_nonsmoker) / (p_sample_var_w )

t_stat_1 <- qt(alpha / 2, dof_1)

p_value_obs_1 <- dt(t_obs_1, dof_1)

#Two Sample T-test - Difference Variance Sample Variance - P-value

dof_2 <- satterth(var_smoker, var_nonsmoker, n_smoker, n_nonsmoker)

np_sample_var_2 <- (var_nonsmoker/n_smoker + var_nonsmoker/n_nonsmoker)

t_obs_2 <- (mu_smoker - mu_nonsmoker) / (sqrt(var_nonsmoker/n_smoker + var_nonsmoker/n_nonsmoker))

t_stat_2 <- qt(alpha / 2, dof_2)

p_value_obs_2 <- dt(t_obs_2, dof_2)

# Confidence Limits

diff_mu <- mu_smoker - mu_nonsmoker

#Pooled Sample variance

CL_pooled <- t_stat_1 * p_sample_var_w 

#Non pooled Sample variance

CL_nonpooled <- t_stat_2 * (sqrt(var_nonsmoker/n_smoker + var_nonsmoker/n_nonsmoker))

CI_pooled <- round(c(diff_mu + CL_pooled, diff_mu - CL_pooled), 2)

CI_nonpooled <-round(c(diff_mu + CL_nonpooled, diff_mu - CL_nonpooled), 2)

#Power Calculation assuming delta means is the true delta

cv_lo_p <- qnorm(alpha / 2, 0, sqrt(p_sample_var_1/n_smoker + 
                                      p_sample_var_1/n_nonsmoker))
cv_hi_p <- qnorm(1 - alpha / 2, 0, sqrt(p_sample_var_1/n_smoker + 
                                          p_sample_var_1/n_nonsmoker))

power1 <- pnorm(cv_lo_p, (mu_smoker - mu_nonsmoker), 
                sqrt(p_sample_var_1/n_smoker + p_sample_var_1/n_nonsmoker))
power2 <- 1 - pnorm(cv_hi_p, (mu_smoker - mu_nonsmoker), 
                    sqrt(p_sample_var_1/n_smoker + p_sample_var_1/n_nonsmoker))

power_pooled <- sum(power1, power2)

cv_lo_non <- qnorm(alpha / 2, 0, 
               (sqrt(var_nonsmoker/n_smoker + var_nonsmoker/n_nonsmoker)))
cv_hi_non <- qnorm(1 - alpha / 2, 0, 
               (sqrt(var_nonsmoker/n_smoker + var_nonsmoker/n_nonsmoker)))

power1 <- pnorm(cv_lo_non, (mu_smoker - mu_nonsmoker), 
                (sqrt(var_nonsmoker/n_smoker + var_nonsmoker/n_nonsmoker)))
power2 <- 1 - pnorm(cv_hi_non, (mu_smoker - mu_nonsmoker), 
                    (sqrt(var_nonsmoker/n_smoker + var_nonsmoker/n_nonsmoker)))

power_nonpooled <- sum(power1, power2)

```

To begin, we first found the difference in mean blood pressure between smokers and nonsmokers in this dataset; this was calculated to be `r round((mu_smoker - mu_nonsmoker), 2)`. Despite this value being nonzero, we cannot safely assume the difference occurred other than by random chance. 
For this part of the analysis we assumed equal variances, thereby allowing us to use pooled sample variance (see Appendix I: Equation 1)  which was calculated to be `r round(p_sample_var_1, 1)`. Under these conditions, the p value (probability of obtaining the observed results) was calculated to be `r round(p_value_obs_1, 4)`, using 298 degrees of freedom, which is smaller than  (probability of committing a Type I error) of 0.05. In terms of t values, our observed t value of `r round(t_obs_1, 2)` is smaller than the t value, `r round(t_stat_1, 2)` for a two sided $\alpha$  of 0.05. 

Additionally, the 95% confidence interval was calculated under the pooled variance assumption. The observed interval is `r CI_pooled[1]` to `r CI_pooled[2]`. One can see that zero does not fall into this interval. Both calculations (p-value and 95% confidence interval) provides enough evidence to allow for the rejection of the null hypothesis. In other words, at an  level of 0.05, there is significant evidence to suggest this group of smokers and nonsmokers have a different range of systolic blood pressure when using pooled variance.

Repeating the same approach as before, we then assumed unequal population variances. In this case, the computed sample variances of the two groups were `r round(var_smoker, 1)` for smokers and `r round(var_nonsmoker, 1)` for nonsmokers. Using equation 3 from Appendix 1, an observed t value of `r round(t_obs_2, 1)` was obtained. Comparing that the two-sided $\alpha$ of `r round(t_stat_2, 2)` with `r dof_2` degrees of freedom (as computed using the Satterthwaite Approximation; see Appendix I: Equation 4), the observed t value was less than the chosen $\alpha$ value of 0.05, as in the case of pooled sample variance.

Furthermore, the 95% confidence interval in the unequal variance case was found to be `r CI_nonpooled[1]` to `r CI_nonpooled[2]`. It can be noted that zero is absent from this range, as in the case of the pooled sample variance. In this second scenario, there is sufficient evidence to reject the null hypothesis and to support the alternative at an $\alpha$ level of 0.05.

We have shown that there is sufficient evidence to support the systolic blood pressure between smokers and nonsmokers is different. This statement holds true in all cases described in this paper. It should be noted, that while the observed systolic blood pressure values vary between smokers and non smokers, we do not know the underlying cause based on the provided data.

### Test Preference

Here, both tests allow us to reject the null hypothesis, so either one could be useful. In this case, we must allow the computed statistics to dictate which test we ought to prefer. The standard deviation (pooled variance) was calculated from the denominator of Equation 2 (see Appendix I) which returned a value of `r round(p_sample_var_w, 2)`. Similarly, the standard deviation (unequal variance) was calculated from the denominator of Equation 3 which returned a value of `r round(np_sample_var_2, 2)`. We want to have data that is as consistent as possible, in other words, with the smallest spread, i.e., variance; therefore, we prefer the test with pooled variance.


\newpage

## Part II

### Introduction

In Part I, we were able to see how hypothesis testing allowed us to produce enough evidence to challenge the status quo argument that “smoking has no effect on the blood pressure of a person.” We found, through a number of robust statistical methods, the difference in means (calculated to be `r round(diff_mu, 2)`) was significant enough to not have arisen merely by chance. Within an $\alpha$ of 5%, we are confident that the two groups do exhibit measurable differences. In the following sections, we will explore, through simulated data, the concept of randomness in data and what steps we can take to mitigate the effect of randomness. All simulated data will be generated using the built-in rnorm() function in R.

### Power of Hypothesis Testing

It is important to briefly discuss the concept of power and, by extension, error types in hypothesis testing. Power describes the probability of not committing a Type II error, which is to not reject the null hypothesis when there was, in actuality, enough evidence to do so. The probability of a Type II error is represented by $\beta$. Power is the complement to that probability (1 - $\beta$) and can be thought of as “test sensitivity.” The more powerful the test, the more sensitive it is to differences between distributions. The value of $\beta$ is the portion of the alternative distribution that is within the null hypothesis non-rejection region limits. Below are a number of plots to help depict this concept. In each of the plots, the shaded red is the null distribution and its location stays constant (mean = 3). The dark tails represent the rejection region of the null distribution and correspond to the size of $\alpha$ (in this case 2.5% on each side); $\alpha$ also indicates the probability of committing a Type I error, which is rejecting the null hypothesis incorrectly. The outline distribution represents the "true" alternative distribution. Within that distribution is the shaded light blue region which is equal to $\beta$. One can see that as the difference between the two distributions shrinks, the value of $\beta$ increases and the power shrinks. Power reaches minimum and $\beta$ reaches maximum when the two distributions are the same. It should be noted that unless the null and alternative distributions are exact copies of one another, there is always a nonzero probability of committing a Type I error.


```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.dim = c(6, 2) ,fig.align = 'center', cache = TRUE}

#Part II
#Introduction

#options(repr.plot.width = 12, repr.plot.height = 3, repr.plot.res = 150)
set.seed(100)

null_mean <- 3
alt_means <- c(0, 3, 5)
plot_list <- list()

#plot_colors <- c("#072ac8","#1e96fc","#a2d6f9","#fcf300","#ffc600")

for(i in 1:length(alt_means)){
    
    sim1 <- rnorm(5000, null_mean, sqrt(1))
    sim2 <- rnorm(5000, alt_means[i], sqrt(1))

    alpha1 <- qnorm(0.025, null_mean, sqrt(1)) 
    alpha2 <- qnorm(0.975, null_mean, sqrt(1))

    df_set <- tibble("H0" = sim1, "HA" = sim2)
    
    title_string <- sprintf("Difference in Means %i", (alt_means[i] - null_mean))

    plot_list[[i]]  <-
    ggplot(data = df_set) + geom_density(aes(H0), alpha = 0.5, fill = plot_colors[5]) + 
                            geom_area(
                                aes(x = stage(H0, after_scale = oob_censor(x, c(-Inf, alpha1)
                                                                          )
                                             )
                                   ),
                                stat = "density", fill = plot_colors[1]
                                 ) +
                            geom_area(
                                aes(x = stage(H0, after_scale = oob_censor(x, c(alpha2, Inf)
                                                                          )
                                             )
                                   ),
                                stat = "density", fill = plot_colors[1]
                                     ) +
                            geom_density(aes(HA), alpha = 0.5) + 
                            geom_area(
                                aes(x = stage(HA, after_scale = oob_censor(x, c(alpha1, alpha2)
                                                                          )
                                             )
                                   ),
                                stat = "density", fill = plot_colors[2], alpha = 0.5
                                     ) +
                            xlim(-2, 8) + xlab("") + ylab("") + ggtitle(title_string) +
                            theme(text = element_text(size = 8))
    
                                }

do.call(grid.arrange, c(plot_list, ncol = 3, heights = 0.5))
```

### Simulation Study

Multiple scenarios were simulated by creating two normally distributed data sets of various means, variances, and sample sizes. Each scenario was repeated a thousand times. For each of those repeats, a hypothesis test was conducted and the number of times the null hypothesis was rejected was recorded (see Appendix III for results.) All hypothesis testing used both pooled and unequal variance, regardless of the actual values of variance. 

The following table includes the values of each parameter used in the simulation:

|Parameter|Possible Values|
|-|-|
|$\mu_{1}$|0, 4, 5, 6, 10|
|$\sigma_{1}^{2}$|1, 4, 9|
|$n_{1}$|10, 30, 70|
|$\mu_{2}$|5|
|$\sigma_{2}^{2}$|1|
|$n_{2}$|10, 30, 70|

```{r, echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
#Part II
set.seed(1)

alpha <- 0.05

#function to take two datasets and run a hypothesis test on them. This outputs the test results.

test_function <- function (x, y, pooled = FALSE){

    mu_1 <- mean(x)
    var_1 <- sample_variance(x, sampling = TRUE)
    
    mu_2 <- mean(y)
    var_2 <- sample_variance(y, sampling = TRUE)
    
    #Calculate the pooled sample variance
    pooled_sample <- ((length(x) - 1) * var_1 + 
                          (length(y) - 1) * var_2) / (length(x) + length(y) - 2)
    
    #calculate the observed t statistic
    if (pooled == TRUE){
        
        cal_sigma <- (sqrt(pooled_sample/length(x) + pooled_sample/length(y)))
        
        ttest <- (mu_1 - mu_2) / cal_sigma
        dof <- length(x) + length(y) - 2 #Determine degrees of freedom
        
        } else {
        
        cal_sigma <- (sqrt(var_1/length(x) + var_2/length(y)))
        
        ttest <- (mu_1 - mu_2) / cal_sigma
        dof <- satterth(var_1, var_2, length(x), length(y))
    }
    
    # Determine whether or not the null hypothesis 
    # can be rejected (1 = rejected, 0 = not rejected)
    verdict <- !between(ttest, qt(alpha / 2, dof), qt(1 - alpha / 2, dof))
    
    #Power calculation assuming calculated difference in means is Ha 
    cv_lo <- qnorm(alpha / 2, 0, cal_sigma)
    cv_hi <- qnorm(1 - alpha / 2, 0, cal_sigma)
    
    power1 <- pnorm(cv_lo, (mu_1 - mu_2), cal_sigma)
    power2 <- 1 - pnorm(cv_hi, (mu_1 - mu_2), cal_sigma)
    
    power <- sum(power1, power2)
    
    #Return calculated values
    return(c(mu_1, var_1, mu_2, var_2, ttest, cal_sigma, dof, verdict, power))
}


# Simulation study parameters
mu1 <- c(0, 4, 5, 6, 10)
var1 <- c(1, 4, 9)
n1 <- c(10, 30, 70)

mu2 <- 5
var2 <- 1
n2 <- c(10, 30, 70)

sim_test <- function(x_mu, x_var, x_n, y_mu, y_var, y_n, pooled){
    
    sim_data_results <- matrix(rep(0, 9), ncol = 9)
    
    for (i in 1:1000){
        
        #create simulation test sets
        sim_set1 <- rnorm(x_n, x_mu, sqrt(x_var))
        sim_set2 <- rnorm(y_n, y_mu, sqrt(y_var))
        
        sim_data_results <- rbind(sim_data_results, 
                                  test_function(sim_set1, sim_set2, pooled))
        
        #print(sim_data_results)
    }
    
    df_sim_data <- data.frame(sim_data_results[2 : nrow(sim_data_results),])
    colnames(df_sim_data) = c("Null Mean", "Null Variance", "Alternate Mean", 
                              "Alternate Variance", "T statistic", 
                              "Calculated Variance", "DoF", "Null Reject", 
                              "Power")
    return(df_sim_data)
}

# All possible combinations of study conditions
df_combo <- expand.grid(mu1, var1, n1, mu2, var2, n2)

# Final table of conditions and results
df_combo2 <- tibble(cbind(1:nrow(df_combo), df_combo, 
                          matrix(rep(0, 2 * nrow(df_combo)), ncol = 2))) 
colnames(df_combo2) <- c("Test_Case", "mu1", "var1", "n1", "mu2", "var2", 
                         "n2", "Test_Results_up", "Test_Results_po")

test_results <- list() # Pooled assumption raw data
test_results2 <- list() # unequal variance assumption raw data

# Filling out raw data and final table with test results
for (i in 1:nrow(df_combo)){
    test_results[[i]] <- do.call(sim_test, 
                                 as.list(as.numeric(c(df_combo[i,],
                                                    pooled = FALSE))))
    test_results2[[i]] <- do.call(sim_test, 
                                  as.list(as.numeric(c(df_combo[i,], 
                                                       pooled = TRUE))))
    df_combo2[i, 8] <- sum(as.data.frame(test_results[i])[,8])
    df_combo2[i, 9] <- sum(as.data.frame(test_results2[i])[,8])    
    }

df_combo2 <- df_combo2 %>% mutate(diff = mu1 - mu2)
#df_combo2 %>%  head()

# Some summary statitics to look for relationships in the attributes
av_med_stats_by_diff <- 
    df_combo2 %>% group_by(diff) %>% summarise(avg_up = mean(Test_Results_up), 
                                               med_up = median(Test_Results_up), 
                                               avg_po = mean(Test_Results_po), 
                                               med_po = median(Test_Results_po))


av_med_stats_by_var <- 
    df_combo2 %>% group_by(var1) %>% summarise(avg_up = mean(Test_Results_up), 
                                               med_up = median(Test_Results_up), 
                                               avg_po = mean(Test_Results_po), 
                                               med_po = median(Test_Results_po))

av_med_stats_by_nulln <- 
    df_combo2 %>% group_by(n1) %>% summarise(avg_up = mean(Test_Results_up), 
                                             med_up = median(Test_Results_up), 
                                             avg_po = mean(Test_Results_po), 
                                             med_po = median(Test_Results_po))

av_med_stats_by_altn <- 
    df_combo2 %>% group_by(n2) %>% summarise(avg_up = mean(Test_Results_up), 
                                             med_up = median(Test_Results_up), 
                                             avg_po = mean(Test_Results_po), 
                                             med_po = median(Test_Results_po))

av_med_stats_by_comn <- 
    df_combo2 %>% mutate(comn = n1 + n2) %>% group_by(comn) %>% 
    summarise(avg_up = mean(Test_Results_up), 
              med_up = median(Test_Results_up), 
              avg_po = mean(Test_Results_po), 
              med_po = median(Test_Results_po))

```

### Observations

It was noted that the smaller difference between the alternative and null mean values were, the less likely the null hypothesis was rejected (see plot below and for more detail Appendix IV Item A). Larger values of variance also reduced the number of rejected null hypotheses (see Appendix IV Item B). In both cases, the more overlap between the two distributions, the less powerful the test. It was observed that increasing the sample size increased power (see Appendix IV Item C, D, and E). For a specific example, we can look at test cases #9 and #129 where $\mu_{1}$ = 6, $\mu_{2}$ = 5, $\sigma^{2}_{1}$ = 4 and $\sigma^{2}_{2}$ = 1. The two cases differed in sample size where $n_{9}$ = 10 and $n_{129}$ = 70.This increase in sample size resulted in an almost fourfold increase in power [271 vs 951 (pooled) or 265 vs 948 (unequal) rejected nulls]. It stands to reason that large variances and small deltas in mean can be mitigated by increasing the sample size accordingly. 

```{r, echo = FALSE, fig.dim = c(5.5, 2.5) , fig.align = 'center'}
av_med_stats_by_diff %>% gather(key = "stat", value = "calc", avg_up:med_po) %>% 
                         ggplot() + geom_line(aes(x = diff, y = calc, color = stat)) + 
                                    geom_point(aes(x = diff, y = calc, color = stat)) + 
                                    xlab("Difference in Means") +
                                    ylab("Null Reject Statistics") + 
                                    scale_x_continuous(breaks = seq(-5, 5, 1)) +
                                    ggtitle("Effect of" ~ Delta ~ "Means on Test Sensitivity") +
                                    scale_color_discrete(name = "Attribute Stat",
                                                         labels = c("Pooled/Mean", 
                                                                    "Unequal/Mean", 
                                                                    "Pooled/Median", 
                                                                    "Unequal/Median")) +
                                    theme(legend.position = c(0.85, 0.3),
                                          title = element_text(size = 8),
                                          legend.text = element_text(size = 6),
                                          legend.key.height = unit(0.25, 'cm'),
                                          legend.key.width = unit(0.25, 'cm')) +
                                    annotate("segment", 
                                             x = c(-1.1, -1.1, 0.25), xend = c(-1.1, -1.1, 0.25), 
                                             y = c(0, 625, 0), yend = c(625, 1000, 50), 
                                             colour = "blue", 
                                             size = 0.5, arrow = arrow(ends = "both", 
                                                                       length = unit(0.1, "cm"),
                                                                       angle = 90)) + 
                                    geom_text(label = "Power", x = -1.75, y = 350, 
                                              size = 2.5, color = "blue") +
                                    geom_text(label = "Type I Error", x = 1.1, y = 25, 
                                              size = 2.5, color = "blue") +
                                    geom_text(label = "Type II\n Error", x = -1.75, y = 825, 
                                              size = 2.5, color = "blue")
```

Type I errors can be clearly observed in the simulated data. All test cases where $\Delta\mu$ was zero, roughly 50 out of the 1000 simulations resulted in rejected null hypotheses (see Appendix IV Item A table). These erroneous results, in roughly 5% of the cases, are due to the difference in variance between the distributions. That percentage value closely matches that of $\alpha$.

One final observation, for each of the test cases (1 - $\beta$) was calculated under the assumption that the observed $\Delta\mu$ was the true distribution. It was found that (1 - $\beta$) values followed a similar pattern to that of the number of rejected null hypotheses, indicating the relationship between power and sensitivity. Below are the calculations for test cases 9 and 129:

```{r, echo = FALSE}

pow_tab_9a <- 
    test_results[[9]] %>% pull(Power) %>% summary() %>% unname() %>% matrix(ncol = 1)
pow_tab_9b <- 
    test_results2[[9]] %>% pull(Power) %>% summary() %>% unname() %>% matrix(ncol = 1)
pow_tab_129a <- 
    test_results[[129]] %>% pull(Power) %>% summary() %>% unname() %>% matrix(ncol = 1)
pow_tab_129b <- 
    test_results2[[129]] %>% pull(Power) %>% summary() %>% unname() %>% matrix(ncol = 1)

d <- cbind(pow_tab_9a, pow_tab_9b, pow_tab_129a, pow_tab_129b)[2:5,]

colnames(d) = c("Test #9 (pooled)", "Test #9 (unequal)", 
                "Test #129 (pooled)", "Test #129 (unequal)")

rownames(d) = c("1st Quartile", "Median", "Mean", "3rd Quartile")

knitr::kable(d)

```

### Conclusion

In Part I, we sought to reject the null hypothesis and determine which testing procedure was preferable by performing t-tests in terms of both p-values and confidence intervals with pooled and unequal variances. The results showed that there is a measurable difference in systolic blood pressure and a test with lower variance is preferable in order to reject the null hypothesis. These results, however, were the end-product of two tests run in two scenarios. To help us better understand the meaning behind the results, it was important to run a numerical simulation study to explore how various factors can effect testing outcomes.

After performing 1,000 trials for each of 135 different scenarios, the data showed that the probability of rejecting the null hypothesis increases with large differences in means or low variances. The data also showed that test sensitivity can be improved by increasing sample sizes. The greatest take away from this simulation study is that increasing the sample size greatly increases the power, or probability of rejecting the null hypothesis when it should, in fact, be rejected. We have also seen that the possibility of committing a Type I error is always present, even for similar distributions.

\newpage

## Contributions

```{r echo = FALSE}
cnames <- c("Halid","Justin")
rnames <- c("template creation", "code", "code review", "write up", "editing", 
            "formatting", "discussion", "calculation verification")

cont<- rbind(
c("75%","25%"), 
c("80%","20%"),
c("50%","50%"),
c("35%","65%"),
c("40%","60%"),
c("30%","70%"),
c("50%","50%"),
c("40%","60%"))

rownames(cont) <- rnames
colnames(cont) <- cnames

knitr::kable(cont)
```

\newpage

## Appendix I: Equations and Figures

### Normality of Data

Plotting the density of total data (Figure 1A), we can see that it closely follows a normal distribution. Density plots of the split data exhibit similar forms (Figure 1B and 1C). We calculated the kurtosis and the skew of the data and found them to `r c(round(data_kurtosis, 3), round(data_skew, 3))` for kurtosis and skew respectively. These values are quite close to those typically found in normally distributed data. Additionally, the calculated values of sample standard deviation, $\frac{IQR^{*}}{1.35}$, and $\frac{MAD^{*}}{0.675}$ are `r round(sqrt(data_samVar), 2)`, `r round(eIQR, 2)`, and `r round(eMAD, 2)`. The similarities between the three values indicate that the data is not heavily influenced by outliers and the Q-Q plot confirms this (See Figure 2). Therefore, we will assume the data and the subsequent split data to be normal.

*Interquartile Range (IQR), Median Absolute Deviation (MAD)

### Figure 1: Data plots
```{r, echo = FALSE, fig.dim = c(7, 2.5), fig.align = 'center'}
# Plotting
options(repr.plot.width = 6, repr.plot.height = 4, repr.plot.res = 150)

plot_grid(total_data, sep_data, plot_3, align = 'vh', 
          hjust = -1,nrow = 1, ncol = 3, labels = c("A", "B", "C"))
```

### Figure 2: Q-Q Plot

```{r, echo = FALSE, fig.dim = c(5, 3)}
data_qqplot + theme_bw()
```



---

### Equations:

\begingroup
\fontsize{16}{16}\selectfont

### Equation 1: Pooled Sample Variance:
$$
S_{p}^{2} = \frac{(n_{1} - 1)S_{1}^{2} + (n_{2} - 1)S_{2}^{2}}{n_{1} + n_{2} - 2}
$$

### Equation 2: Observed T Statistic (pooled variance):
$$
t_{obs} = \frac{\mu_{1} - \mu_{2}}{S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}}
$$

### Equation 3: Observed T Statistic (distinct variance):
$$
t_{obs} = \frac{\mu_{1} - \mu_{2}}{\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}}
$$

### Equation 4: Satterthwaite Approximation:

$$
\nu = \frac{ \left( \frac{ S_{1}^{2}}{n_{1} } + \frac{ S_{2}^{2}}{n_{2} } \right)^{2} }
           { \frac { \left( \frac{ S_{1}^{2}}{n_{1}} \right)^{2} } { n_{1} -1 } + \frac{ \left( \frac{ S_{2}^{2}}{n_{2}} \right)^{2} } { n_{2} -1 } }
$$
\endgroup

\newpage

## Appendix II: Part I Results

```{r, echo = FALSE}
d1 <- matrix(c(mean(df_data$sysBP), sample_variance(df_data$sysBP), 
              mu_smoker, var_smoker,
              mu_nonsmoker, var_nonsmoker, 
              diff_mu, p_sample_var_1, p_sample_var_w, 
              np_sample_var_2, CI_pooled, CI_nonpooled), ncol = 1)


rownames(d1) <- c("Data Mean", "Data Sample Variance", "Smoker Mean", 
                 "Smoker Variance", 
                 "Nonsmoker Mean", "Nonsmoker Variance", 
                 "Difference in Mean", "Pooled Sample Varance", 
                 "T-Statistic Std Deviation (Pooled)", 
                 "T-Statistic Std Deviation (Unequal)",
                 "CI Pooled Lower", "CI Pooled Upper",
                 "CI Nonpooled Lower", "CI Nonpooled Upper")

d1 %>% knitr::kable(col.names = c("Attribute Value"))
```

\newpage

## Appendix III: Part II Results

### Simulation Data

```{r, echo = FALSE}
knitr::kable(df_combo2, col.names = c("Test Case", 
                                      "$\\mu_1$", 
                                      "$\\sigma_1^2$", 
                                      "$n_1$", 
                                      "$\\mu_2$", 
                                      "$\\sigma_2^2$", 
                                      "$n_2$",
                                      "# of Rejected Null (Unequal $\\sigma^{2}$)",
                                      "# of Rejected Null (Pooled $\\sigma^{2}$)",
                                      "Difference in Mean"), 
             escape = FALSE)
```

\newpage

## Appendix IV: Part II Summary

### Item A

```{r, echo = FALSE, fig.dim = c(7, 3)}
av_med_stats_by_diff %>% gather(key = "stat", value = "calc", avg_up:med_po) %>% 
                         ggplot() + geom_line(aes(x = diff, y = calc, color = stat)) + 
                                    geom_point(aes(x = diff, y = calc, color = stat)) + 
                                    xlab("Difference in Means") +
                                    ylab("Null Reject Statistics") + 
                                    ggtitle("Effect of" ~ Delta ~ "Means on Test Sensitivity") +
                                    scale_x_continuous(breaks = seq(-5, 5, 1)) + 
                                    scale_color_discrete(name = "Attribute Stat",
                                                         labels = c("Pooled/Mean", 
                                                                    "Unequal/Mean", 
                                                                    "Pooled/Median", 
                                                                    "Unequal/Median")
                                                         ) +
                                    theme(#legend.position = c(0.85, 0.3),
                                          legend.text = element_text(size = 8),
                                          legend.key.height = unit(0.25, 'cm'),
                                          legend.key.width = unit(0.4, 'cm'))

knitr::kable(av_med_stats_by_diff, col.names = c("$\\Delta$ Means",
                                                 "Average # Rejects (Unequal)",
                                                 "Median # Rejects (Unequal)",
                                                 "Average # Rejects (Pooled)",
                                                 "Median # Rejects (Pooled)"), 
             escape = FALSE)
```

### Item B

```{r, echo = FALSE, fig.dim = c(7, 3)}
av_med_stats_by_var %>% gather(key = "stat", value = "calc", avg_up:med_po) %>% 
                         ggplot() + geom_line(aes(x = var1, y = calc, color = stat)) + 
                                    geom_point(aes(x = var1, y = calc, color = stat)) + 
                                    xlab("Sample Variance") +
                                    ylab("Null Reject Statistics") + 
                                    ggtitle("Effect of Variance on Test Sensitivity") +
                                    scale_x_continuous(breaks = seq(-5, 10, 1)) +
                                    theme(#legend.position = c(0.85, 0.8),
                                          legend.text = element_text(size = 8),
                                          legend.key.height = unit(0.25, 'cm'),
                                          legend.key.width = unit(0.4, 'cm')) +
                                    scale_color_discrete(name = "Attribute Stat",
                                                         labels = c("Pooled/Mean", 
                                                                    "Unequal/Mean", 
                                                                    "Pooled/Median", 
                                                                    "Unequal/Median")
                                                         )

knitr::kable(av_med_stats_by_var, col.names = c("Sample Variance",
                                                 "Average # Rejects (Unequal)",
                                                 "Median # Rejects (Unequal)",
                                                 "Average # Rejects (Pooled)",
                                                 "Median # Rejects (Pooled)"), 
             escape = FALSE)

```

### Item C

```{r, echo = FALSE, fig.dim = c(7, 3)}
av_med_stats_by_nulln %>% gather(key = "stat", value = "calc", avg_up:med_po) %>% 
                          ggplot() + geom_line(aes(x = n1, y = calc, color = stat)) + 
                                     geom_point(aes(x = n1, y = calc, color = stat)) + 
                                     xlab("Null Sample Size") +
                                     ylab("Null Reject Statistics") +
                                     ggtitle("Effect of Null Sample Size on Test Sensitivity") +
                                     scale_x_continuous(breaks = seq(10, 80, 10)) + 
                                     theme(#legend.position = c(0.85, 0.65),
                                           legend.text = element_text(size = 8),
                                           legend.key.height = unit(0.25, 'cm'),
                                           legend.key.width = unit(0.4, 'cm')) +
                                     scale_color_discrete(name = "Attribute Stat",
                                                         labels = c("Pooled/Mean", 
                                                                    "Unequal/Mean", 
                                                                    "Pooled/Median", 
                                                                    "Unequal/Median"))



knitr::kable(av_med_stats_by_nulln, col.names = c("Null Sample Size",
                                                 "Average # Rejects (Unequal)",
                                                 "Median # Rejects (Unequal)",
                                                 "Average # Rejects (Pooled)",
                                                 "Median # Rejects (Pooled)"), 
             escape = FALSE)

```

### Item D

```{r, echo = FALSE, fig.dim = c(7, 3)}
av_med_stats_by_altn %>% gather(key = "stat", value = "calc", avg_up:med_po) %>% 
                          ggplot() + geom_line(aes(x = n2, y = calc, color = stat)) + 
                                     geom_point(aes(x = n2, y = calc, color = stat)) + 
                                     xlab("Alternative Sample Size") +
                                     ylab("Null Reject Statistics") + 
                                     ggtitle("Effect of Alternative Sample Size on Test Sensitivity") +
                                     scale_x_continuous(breaks = seq(10, 80, 10)) + 
                                     theme(#legend.position = c(0.85, 0.65),
                                           legend.text = element_text(size = 8),
                                           legend.key.height = unit(0.25, 'cm'),
                                           legend.key.width = unit(0.4, 'cm')) +
                                     scale_color_discrete(name = "Attribute Stat",
                                                         labels = c("Pooled/Mean", 
                                                                    "Unequal/Mean", 
                                                                    "Pooled/Median", 
                                                                    "Unequal/Median"))


knitr::kable(av_med_stats_by_altn, col.names = c("Alternative Sample Size",
                                                 "Average # Rejects (Unequal)",
                                                 "Median # Rejects (Unequal)",
                                                 "Average # Rejects (Pooled)",
                                                 "Median # Rejects (Pooled)"), 
             escape = FALSE)

```

### Item E

```{r, echo = FALSE, fig.dim = c(7, 3)}

av_med_stats_by_comn %>% gather(key = "stat", value = "calc", avg_up:med_po) %>% 
                          ggplot() + geom_line(aes(x = comn, y = calc, color = stat)) + 
                                     geom_point(aes(x = comn, y = calc, color = stat)) + 
                                     xlab("Total Sample Size") +
                                     ylab("Null Reject Statistics") + 
                                     ggtitle("Effect of Total Sample Size on Test Sensitivity") +
                                     scale_x_continuous(breaks = seq(10, 150, 10)) + 
                                     theme(#legend.position = c(0.85, 0.7),
                                           legend.text = element_text(size = 8),
                                           legend.key.height = unit(0.25, 'cm'),
                                           legend.key.width = unit(0.4, 'cm')) +
                                     scale_color_discrete(name = "Attribute Stat",
                                                         labels = c("Pooled/Mean", 
                                                                    "Unequal/Mean", 
                                                                    "Pooled/Median", 
                                                                    "Unequal/Median"))

knitr::kable(av_med_stats_by_comn, col.names = c("Combined Sample Size",
                                                 "Average # Rejects (Unequal)",
                                                 "Median # Rejects (Unequal)",
                                                 "Average # Rejects (Pooled)",
                                                 "Median # Rejects (Pooled)"), 
             escape = FALSE)

```

\newpage

## Appendix V: Code
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```

